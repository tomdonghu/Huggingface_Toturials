{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Valence_Mean</th>\n",
       "      <th>Arousal_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>怪八卦</td>\n",
       "      <td>3.450</td>\n",
       "      <td>4.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>更加小心</td>\n",
       "      <td>5.111</td>\n",
       "      <td>7.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>格外小心</td>\n",
       "      <td>3.950</td>\n",
       "      <td>6.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>特別小心</td>\n",
       "      <td>5.000</td>\n",
       "      <td>7.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>極為小心</td>\n",
       "      <td>3.989</td>\n",
       "      <td>6.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>沒有最爛</td>\n",
       "      <td>4.144</td>\n",
       "      <td>4.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>沒很爛</td>\n",
       "      <td>4.643</td>\n",
       "      <td>4.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>沒有最蠢</td>\n",
       "      <td>3.214</td>\n",
       "      <td>5.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>沒太驚訝</td>\n",
       "      <td>4.963</td>\n",
       "      <td>5.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>沒有太驚訝</td>\n",
       "      <td>4.878</td>\n",
       "      <td>5.713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Phrase  Valence_Mean  Arousal_Mean\n",
       "0       怪八卦         3.450         4.383\n",
       "1      更加小心         5.111         7.188\n",
       "2      格外小心         3.950         6.500\n",
       "3      特別小心         5.000         7.000\n",
       "4      極為小心         3.989         6.925\n",
       "...     ...           ...           ...\n",
       "2245   沒有最爛         4.144         4.850\n",
       "2246    沒很爛         4.643         4.878\n",
       "2247   沒有最蠢         3.214         5.200\n",
       "2248   沒太驚訝         4.963         5.975\n",
       "2249  沒有太驚訝         4.878         5.713\n",
       "\n",
       "[2250 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVAP_all_SD_df = pd.read_csv('./ChineseEmoBank/CVAP_SD/CVAP_all_SD.csv', encoding= 'utf-8',sep=\"\\t\")\n",
    "df0 = CVAP_all_SD_df.drop(['No.','Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df0['class'] = 'Phrase'\n",
    "#df0.columns = ['Phrase', 'Valence_Mean', 'Arousal_Mean', 'class']\n",
    "\n",
    "CVAS_all_SD_df = pd.read_csv('./ChineseEmoBank/CVAS_SD/CVAS_all.csv', encoding= 'utf-8',sep=\"\\t\")\n",
    "df1 = CVAS_all_SD_df.drop(['Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df1['class'] = 'Text'\n",
    "df1.columns = ['Phrase', 'Valence_Mean', 'Arousal_Mean', 'class']\n",
    "\n",
    "\n",
    "CVAW_all_SD_df = pd.read_csv('./ChineseEmoBank/CVAW_SD/CVAW_all_SD.csv', encoding= 'utf-8',sep=\"\\t\")\n",
    "df2 = CVAW_all_SD_df.drop(['No.','Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df2['class'] = 'Word'\n",
    "df2.columns = ['Phrase', 'Valence_Mean', 'Arousal_Mean', 'class']\n",
    "\n",
    "\n",
    "df = CVAP_all_SD_df.drop(['No.','Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df\n",
    "# print(df0.loc[1])\n",
    "# print(df1.loc[1])\n",
    "# print(df2.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# combined_df = pd.concat([df0, df1, df2], ignore_index=True)\n",
    "\n",
    "# # 将'class'列转换为哑变量\n",
    "# dummy_class = pd.get_dummies(combined_df['class'], prefix='class')\n",
    "\n",
    "# # 将哑变量与原始数据帧合并\n",
    "# combined_df = pd.concat([combined_df, dummy_class], axis=1)\n",
    "\n",
    "# # 删除原始的'class'列\n",
    "# combined_df = combined_df.drop('class', axis=1)\n",
    "# df = combined_df\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['class_Phrase', 'class_Text', 'class_Word'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 提取特徵和標籤\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#X = df[['Valence_Mean', 'Arousal_Mean']]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#y = df['Phrase'] # 如果您的數據集中有標籤列，請替換 'label_column_name' 為您的標籤列名稱\u001b[39;00m\n\u001b[0;32m      4\u001b[0m x \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mPhrase\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m y \u001b[39m=\u001b[39m df[[\u001b[39m'\u001b[39;49m\u001b[39mValence_Mean\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mArousal_Mean\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mclass_Phrase\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mclass_Text\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mclass_Word\u001b[39;49m\u001b[39m'\u001b[39;49m]] \u001b[39m# 如果您的數據集中有標籤列，請替換 'label_column_name' 為您的標籤列名稱\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# 將數據集分成訓練集和測試集，以 80:20 的比例分割\u001b[39;00m\n\u001b[0;32m      8\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(x, y, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, train_size\u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5793\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5796\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5798\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5800\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:5859\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5858\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5859\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['class_Phrase', 'class_Text', 'class_Word'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "# 提取特徵和標籤\n",
    "#X = df[['Valence_Mean', 'Arousal_Mean']]\n",
    "#y = df['Phrase'] # 如果您的數據集中有標籤列，請替換 'label_column_name' 為您的標籤列名稱\n",
    "x = df['Phrase']\n",
    "# y = df[['Valence_Mean','Arousal_Mean','class_Phrase','class_Text','class_Word']] # 如果您的數據集中有標籤列，請替換 'label_column_name' 為您的標籤列名稱\n",
    "y = df['Valence_Mean','Arousal_Mean']\n",
    "# 將數據集分成訓練集和測試集，以 80:20 的比例分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, train_size= 0.8)\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-chinese', return_dict=False)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# from transformers import (\n",
    "#   BertTokenizerFast,\n",
    "#   AutoModel,\n",
    "# )\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "# bert = AutoModel.from_pretrained('ckiplab/bert-base-chinese', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = x_train.dropna().index\n",
    "test_idx = x_test.dropna().index\n",
    "\n",
    "train_tokens = tokenizer.batch_encode_plus(x_train[train_idx].to_list(),\n",
    "                                           max_length = 50,\n",
    "                                           #pad_to_max_length = True,\n",
    "                                           padding=True,\n",
    "                                           truncation = True)\n",
    "test_tokens = tokenizer.batch_encode_plus(x_test[test_idx].to_list(),\n",
    "                                           max_length = 50,\n",
    "                                          # pad_to_max_length = True,\n",
    "                                           padding=True,\n",
    "                                           truncation = True)\n",
    "#print(y_train['Valence_Mean'])\n",
    "# y_train = y_train.reset_index(drop = True)\n",
    "#y_train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(train_tokens['input_ids'])\n",
    "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
    "# print([i for i in y_train['Valence_Mean']])\n",
    "# train_y = torch.tensor([i for i in y_train['Valence_Mean']])\n",
    "train_y = torch.tensor([(i, j) for i, j in zip(y_train['Valence_Mean'], y_train['Arousal_Mean'])])\n",
    "test_seq = torch.tensor(test_tokens['input_ids'])\n",
    "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
    "#test_y = torch.tensor([i for i in y_test['Valence_Mean']])\n",
    "test_y = torch.tensor([(i, j) for i, j in zip(y_test['Valence_Mean'], y_test['Arousal_Mean'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "trainloader = DataLoader(train_data, \n",
    "                         sampler = train_sampler,\n",
    "                         batch_size = 32)\n",
    "\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "testloader = DataLoader(test_data, \n",
    "                         sampler = test_sampler,\n",
    "                         batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertRegressor(nn.Module):\n",
    "#     def __init__(self, bert):\n",
    "#         super().__init__()\n",
    "#         self.bert = bert\n",
    "#         self.fc1 = nn.Linear(768, 1)  # output one continuous value\n",
    "    \n",
    "#     def forward(self, sent_id, mask):\n",
    "#         _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "#         return self.fc1(cls_hs).squeeze()  # remove the last dimension of size 1\n",
    "    \n",
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.fc1 = nn.Linear(768, 128)  # add a linear layer with output size 128\n",
    "        self.relu = nn.ReLU()  # add ReLU activation function\n",
    "        self.fc2 = nn.Linear(128, 128)  # output one continuous value\n",
    "        self.fc3 = nn.Linear(128, 2)  # output one continuous value\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)  # apply ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)  # apply ReLU activation\n",
    "        # return self.fc3(x).squeeze()  # remove the last dimension of size 1\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressor(bert)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(outputs, targets):\n",
    "#     return F.mse_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss()\n",
    "#criterion = nn.MSELoss(reduction='sum')\n",
    "criterion = torch.nn.SmoothL1Loss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# def early_stopping(train_loss, validation_loss, min_delta, tolerance):\n",
    "\n",
    "#     counter = 0\n",
    "#     if (validation_loss - train_loss) > min_delta:\n",
    "#         counter +=1\n",
    "#         if counter >= tolerance:\n",
    "#           return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "losses = []\n",
    "for e in range(epochs):   \n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(trainloader):\n",
    "        batch = [i.cuda() for i in batch]\n",
    "        sent_id, masks, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(sent_id, masks)\n",
    "        loss = criterion(preds, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "    losses.append(train_loss / len(trainloader))\n",
    "    # if early_stopping(epoch_train_loss, epoch_validate_loss, min_delta=10, tolerance = 20):\n",
    "    #   print(\"We are at epoch:\", i)\n",
    "    #   break\n",
    "    print(f'Epoch:{e+1}\\t\\tTraining Loss: {train_loss / len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses, label='train_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_label = []\n",
    "# true_label = []\n",
    "# for batch in tqdm(testloader):\n",
    "#     batch = [i.cuda() for i in batch]\n",
    "#     sent_id, masks, labels = batch\n",
    "\n",
    "#     preds = model(sent_id, masks)\n",
    "#     #pred_label.extend(torch.argmax(preds, axis = 1).cpu())\n",
    "#     pred_label.extend(preds.cpu())\n",
    "#     true_label.extend(labels.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在上面的代码中，我们首先定义了两个空列表pred_label和true_label来存储\n",
    "#模型的预测标签和真实标签。然后，我们遍历测试集并对每个批次进行预测\n",
    "#。将预测值和真实值添加到相应的列表中后，我们可以使用\n",
    "#sklearn库中的mean_absolute_error函数来计算MAE。\n",
    "#最后，我们将MAE打印出来。\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pred_label = []\n",
    "# true_label = []\n",
    "pred_label_1 = []\n",
    "pred_label_2 = []\n",
    "true_label_1 = []\n",
    "true_label_2 = []\n",
    "for batch in tqdm(testloader):\n",
    "    batch = [i.cuda() for i in batch]\n",
    "    sent_id, masks, labels = batch\n",
    "\n",
    "    preds = model(sent_id, masks)\n",
    "    # pred_label.extend(preds.detach().cpu().numpy())\n",
    "    # true_label.extend(labels.detach().cpu().numpy())\n",
    "    pred_label_1.extend(preds[:, 0].detach().cpu().numpy())\n",
    "    pred_label_2.extend(preds[:, 1].detach().cpu().numpy())\n",
    "    true_label_1.extend(labels[:, 0].detach().cpu().numpy())\n",
    "    true_label_2.extend(labels[:, 1].detach().cpu().numpy())\n",
    "\n",
    "# mae = mean_absolute_error(true_label, pred_label)\n",
    "mae_1 = mean_absolute_error(true_label_1, pred_label_1)\n",
    "mae_2 = mean_absolute_error(true_label_2, pred_label_2)\n",
    "\n",
    "#print(f'MAE: {mae}')\n",
    "\n",
    "print(f'MAE for Valence: {mae_1}')\n",
    "print(f'MAE for Arousal: {mae_2}')\n",
    "#Ckipall2000,MAE for Valence: 0.9184504151344299,MAE for Arousal: 1.0241398811340332\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(sentence, model, tokenizer):\n",
    "#     encoded_sent = tokenizer.encode_plus(\n",
    "#         sentence,\n",
    "#         truncation=True,\n",
    "#         max_length=50,\n",
    "#         add_special_tokens=True,\n",
    "#         # pad_to_max_length=True,\n",
    "#         padding='longest',\n",
    "#         return_attention_mask=True,\n",
    "#         return_tensors='pt'\n",
    "#     )\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     input_ids = encoded_sent['input_ids'].to(device)\n",
    "#     attention_mask = encoded_sent['attention_mask'].to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_ids, attention_mask)\n",
    "\n",
    "#     return output.item()\n",
    "\n",
    "def predict_sentiment(sentence, model, tokenizer):\n",
    "    encoded_sent = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        max_length=50,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cuda'if torch.cuda.is_available() else print(\"gpu error\"))\n",
    "    input_ids = encoded_sent['input_ids'].to(device)\n",
    "    attention_mask = encoded_sent['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    return output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"傻逼嗎?\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你害我輸了\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"可惡，你害我放槍了\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"就等你這張\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"不知道要出哪張\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你到底會不會打牌\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"難過\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你很討厭\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你老是盯我的牌，讓我很不舒服。\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"我放槍了，這場遊戲讓我很失望。\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"很不友善\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"太可惡了\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"讓我多想想呢\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"我很高興\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"我高興到不行\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"生氣\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../DongModel/BaseAll1000DummySmooth')\n",
    "model.load_state_dict(torch.load('../DongModel/CkipAll1000DummySmooth'))\n",
    "model.eval()\n",
    "sentence = \"胡屁胡\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46ba907fca5953e6721f86052b9f8575b23c87f1d6e8c828f5acc984b33e1030"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
