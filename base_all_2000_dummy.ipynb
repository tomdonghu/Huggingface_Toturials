{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVAP_all_SD_df = pd.read_csv('./ChineseEmoBank/CVAP_SD/CVAP_all_SD.csv', encoding= 'utf-8',sep=\"\\t\")\n",
    "df0 = CVAP_all_SD_df.drop(['No.','Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df0['class'] = 'Phrase'\n",
    "#df0.columns = ['Phrase', 'Valence_Mean', 'Arousal_Mean', 'class']\n",
    "\n",
    "CVAS_all_SD_df = pd.read_csv('./ChineseEmoBank/CVAS_SD/CVAS_all.csv', encoding= 'utf-8',sep=\"\\t\")\n",
    "df1 = CVAS_all_SD_df.drop(['Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df1['class'] = 'Text'\n",
    "df1.columns = ['Phrase', 'Valence_Mean', 'Arousal_Mean', 'class']\n",
    "\n",
    "\n",
    "CVAW_all_SD_df = pd.read_csv('./ChineseEmoBank/CVAW_SD/CVAW_all_SD.csv', encoding= 'utf-8',sep=\"\\t\")\n",
    "df2 = CVAW_all_SD_df.drop(['No.','Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "df2['class'] = 'Word'\n",
    "df2.columns = ['Phrase', 'Valence_Mean', 'Arousal_Mean', 'class']\n",
    "\n",
    "\n",
    "#df = CVAP_all_SD_df.drop(['No.','Valence_SD', 'Arousal_SD'], axis= 1)\n",
    "print(df0.loc[1])\n",
    "print(df1.loc[1])\n",
    "print(df2.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([df0,df1,df2], axis= 0).reset_index(drop= True)\n",
    "#df = df0\n",
    "#df.loc[0]\n",
    "combined_df = pd.concat([df0, df1, df2], ignore_index=True)\n",
    "\n",
    "# 将'class'列转换为哑变量\n",
    "dummy_class = pd.get_dummies(combined_df['class'], prefix='class')\n",
    "\n",
    "# 将哑变量与原始数据帧合并\n",
    "combined_df = pd.concat([combined_df, dummy_class], axis=1)\n",
    "\n",
    "# 删除原始的'class'列\n",
    "combined_df = combined_df.drop('class', axis=1)\n",
    "df = combined_df\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 提取特徵和標籤\n",
    "#X = df[['Valence_Mean', 'Arousal_Mean']]\n",
    "#y = df['Phrase'] # 如果您的數據集中有標籤列，請替換 'label_column_name' 為您的標籤列名稱\n",
    "x = df['Phrase']\n",
    "y = df[['Valence_Mean','Arousal_Mean','class_Phrase','class_Text','class_Word']] # 如果您的數據集中有標籤列，請替換 'label_column_name' 為您的標籤列名稱\n",
    "\n",
    "# 將數據集分成訓練集和測試集，以 80:20 的比例分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-chinese', return_dict=False)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# from transformers import (\n",
    "#   BertTokenizerFast,\n",
    "#   AutoModel,\n",
    "# )\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "# bert = AutoModel.from_pretrained('ckiplab/bert-base-chinese', return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = x_train.dropna().index\n",
    "test_idx = x_test.dropna().index\n",
    "\n",
    "train_tokens = tokenizer.batch_encode_plus(x_train[train_idx].to_list(),\n",
    "                                           max_length = 50,\n",
    "                                           #pad_to_max_length = True,\n",
    "                                           padding=True,\n",
    "                                           truncation = True)\n",
    "test_tokens = tokenizer.batch_encode_plus(x_test[test_idx].to_list(),\n",
    "                                           max_length = 50,\n",
    "                                          # pad_to_max_length = True,\n",
    "                                           padding=True,\n",
    "                                           truncation = True)\n",
    "#print(y_train['Valence_Mean'])\n",
    "# y_train = y_train.reset_index(drop = True)\n",
    "#y_train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(train_tokens['input_ids'])\n",
    "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
    "# print([i for i in y_train['Valence_Mean']])\n",
    "# train_y = torch.tensor([i for i in y_train['Valence_Mean']])\n",
    "train_y = torch.tensor([(i, j) for i, j in zip(y_train['Valence_Mean'], y_train['Arousal_Mean'])])\n",
    "test_seq = torch.tensor(test_tokens['input_ids'])\n",
    "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
    "#test_y = torch.tensor([i for i in y_test['Valence_Mean']])\n",
    "test_y = torch.tensor([(i, j) for i, j in zip(y_test['Valence_Mean'], y_test['Arousal_Mean'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "trainloader = DataLoader(train_data, \n",
    "                         sampler = train_sampler,\n",
    "                         batch_size = 32)\n",
    "\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "testloader = DataLoader(test_data, \n",
    "                         sampler = test_sampler,\n",
    "                         batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertRegressor(nn.Module):\n",
    "#     def __init__(self, bert):\n",
    "#         super().__init__()\n",
    "#         self.bert = bert\n",
    "#         self.fc1 = nn.Linear(768, 1)  # output one continuous value\n",
    "    \n",
    "#     def forward(self, sent_id, mask):\n",
    "#         _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "#         return self.fc1(cls_hs).squeeze()  # remove the last dimension of size 1\n",
    "    \n",
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.fc1 = nn.Linear(768, 128)  # add a linear layer with output size 128\n",
    "        self.relu = nn.ReLU()  # add ReLU activation function\n",
    "        self.fc2 = nn.Linear(128, 128)  # output one continuous value\n",
    "        self.fc3 = nn.Linear(128, 2)  # output one continuous value\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)  # apply ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)  # apply ReLU activation\n",
    "        # return self.fc3(x).squeeze()  # remove the last dimension of size 1\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressor(bert)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(outputs, targets):\n",
    "#     return F.mse_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "losses = []\n",
    "for e in range(epochs):   \n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(trainloader):\n",
    "        batch = [i.cuda() for i in batch]\n",
    "        sent_id, masks, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(sent_id, masks)\n",
    "        loss = criterion(preds, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "    losses.append(train_loss / len(trainloader))\n",
    "    print(f'Epoch:{e+1}\\t\\tTraining Loss: {train_loss / len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses, label='train_loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_label = []\n",
    "# true_label = []\n",
    "# for batch in tqdm(testloader):\n",
    "#     batch = [i.cuda() for i in batch]\n",
    "#     sent_id, masks, labels = batch\n",
    "\n",
    "#     preds = model(sent_id, masks)\n",
    "#     #pred_label.extend(torch.argmax(preds, axis = 1).cpu())\n",
    "#     pred_label.extend(preds.cpu())\n",
    "#     true_label.extend(labels.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在上面的代码中，我们首先定义了两个空列表pred_label和true_label来存储\n",
    "#模型的预测标签和真实标签。然后，我们遍历测试集并对每个批次进行预测\n",
    "#。将预测值和真实值添加到相应的列表中后，我们可以使用\n",
    "#sklearn库中的mean_absolute_error函数来计算MAE。\n",
    "#最后，我们将MAE打印出来。\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# pred_label = []\n",
    "# true_label = []\n",
    "pred_label_1 = []\n",
    "pred_label_2 = []\n",
    "true_label_1 = []\n",
    "true_label_2 = []\n",
    "for batch in tqdm(testloader):\n",
    "    batch = [i.cuda() for i in batch]\n",
    "    sent_id, masks, labels = batch\n",
    "\n",
    "    preds = model(sent_id, masks)\n",
    "    # pred_label.extend(preds.detach().cpu().numpy())\n",
    "    # true_label.extend(labels.detach().cpu().numpy())\n",
    "    pred_label_1.extend(preds[:, 0].detach().cpu().numpy())\n",
    "    pred_label_2.extend(preds[:, 1].detach().cpu().numpy())\n",
    "    true_label_1.extend(labels[:, 0].detach().cpu().numpy())\n",
    "    true_label_2.extend(labels[:, 1].detach().cpu().numpy())\n",
    "\n",
    "# mae = mean_absolute_error(true_label, pred_label)\n",
    "mae_1 = mean_absolute_error(true_label_1, pred_label_1)\n",
    "mae_2 = mean_absolute_error(true_label_2, pred_label_2)\n",
    "\n",
    "#print(f'MAE: {mae}')\n",
    "\n",
    "print(f'MAE for Valence: {mae_1}')\n",
    "print(f'MAE for Arousal: {mae_2}')\n",
    "#Ckipall2000,MAE for Valence: 0.9184504151344299,MAE for Arousal: 1.0241398811340332\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(sentence, model, tokenizer):\n",
    "#     encoded_sent = tokenizer.encode_plus(\n",
    "#         sentence,\n",
    "#         truncation=True,\n",
    "#         max_length=50,\n",
    "#         add_special_tokens=True,\n",
    "#         # pad_to_max_length=True,\n",
    "#         padding='longest',\n",
    "#         return_attention_mask=True,\n",
    "#         return_tensors='pt'\n",
    "#     )\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     input_ids = encoded_sent['input_ids'].to(device)\n",
    "#     attention_mask = encoded_sent['attention_mask'].to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_ids, attention_mask)\n",
    "\n",
    "#     return output.item()\n",
    "\n",
    "def predict_sentiment(sentence, model, tokenizer):\n",
    "    encoded_sent = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        max_length=50,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cuda'if torch.cuda.is_available() else print(\"gpu error\"))\n",
    "    input_ids = encoded_sent['input_ids'].to(device)\n",
    "    attention_mask = encoded_sent['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask)\n",
    "\n",
    "    return output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "傻逼嗎? [[-0.84920502 -0.11555433]]\n",
      "你害我輸了 [[-1.42566347  1.28789759]]\n",
      "可惡，你害我放槍了 [[-1.32894135  1.27480507]]\n",
      "就等你這張 [[0.94010544 0.69417572]]\n",
      "不知道要出哪張 [[ 0.79279709 -0.17092657]]\n",
      "你到底會不會打牌 [[-0.89436197  0.35381413]]\n",
      "難過 [[-1.60803008  1.1776185 ]]\n",
      "你很討厭 [[-1.58956313  1.91041517]]\n",
      "你老是盯我的牌，讓我很不舒服。 [[-1.26012301  1.45587492]]\n",
      "我放槍了，這場遊戲讓我很失望。 [[-1.2954464   0.94301939]]\n",
      "很不友善 [[-1.95333695  0.76711369]]\n",
      "太可惡了 [[-3.00373197  1.89846802]]\n",
      "讓我多想想呢 [[-0.42802572  0.29262018]]\n",
      "我很高興 [[1.80184746 0.81332302]]\n",
      "我高興到不行 [[0.59375286 1.41140795]]\n",
      "生氣 [[0.62796402 0.23865461]]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"傻逼嗎?\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你害我輸了\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"可惡，你害我放槍了\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"就等你這張\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"不知道要出哪張\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你到底會不會打牌\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"難過\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你很討厭\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"你老是盯我的牌，讓我很不舒服。\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"我放槍了，這場遊戲讓我很失望。\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"很不友善\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"太可惡了\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"讓我多想想呢\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"我很高興\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"我高興到不行\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "sentence = \"生氣\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(sentence,score-(5,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.920196  6.1342626]]\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), '../DongModel/BaseAll2000')\n",
    "model.load_state_dict(torch.load('../DongModel/BaseAll2000'))\n",
    "model.eval()\n",
    "sentence = \"胡屁胡\"\n",
    "score = predict_sentiment(sentence, model, tokenizer)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46ba907fca5953e6721f86052b9f8575b23c87f1d6e8c828f5acc984b33e1030"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
